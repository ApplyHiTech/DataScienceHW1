{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sqlFunctions\n",
    "# import matplotlib.pyplot as plt\n",
    "import config\n",
    "import summary\n",
    "import features\n",
    "\n",
    "from operator import add\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from etl import CriteoDataSets, CriteoData\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = CriteoDataSets(config.SPARK_CONTEXT, config.SPARK_SQL_CONTEXT)\n",
    "train = data.train if not config.DEBUG else data.debug\n",
    "train_3m = data.train_5m if not config.DEBUG else data.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2], [77337, 22663])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label data histogram\n",
    "summary.label_histogram(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 2:\n",
      "\n",
      "          0.000 - 56: 55,236\n",
      "\n",
      "        55.600 - 111: 295\n",
      "\n",
      "       111.200 - 167: 34\n",
      "\n",
      "       166.800 - 222: 10\n",
      "\n",
      "       222.400 - 278: 5\n",
      "\n",
      "       278.000 - 334: 3\n",
      "\n",
      "       333.600 - 389: 2\n",
      "\n",
      "       389.200 - 445: 0\n",
      "\n",
      "       444.800 - 500: 1\n",
      "\n",
      "       500.400 - 556: 1\n",
      "\n",
      "\n",
      "Column 3:\n",
      "\n",
      "      -2.000 - 1,850: 98,530\n",
      "\n",
      "   1,850.400 - 3,703: 1,397\n",
      "\n",
      "   3,702.800 - 5,555: 52\n",
      "\n",
      "   5,555.200 - 7,408: 12\n",
      "\n",
      "   7,407.600 - 9,260: 7\n",
      "\n",
      "  9,260.000 - 11,112: 0\n",
      "\n",
      " 11,112.400 - 12,965: 1\n",
      "\n",
      " 12,964.800 - 14,817: 0\n",
      "\n",
      " 14,817.200 - 16,670: 0\n",
      "\n",
      " 16,669.600 - 18,522: 1\n",
      "\n",
      "\n",
      "Column 4:\n",
      "\n",
      "       0.000 - 6,554: 80,863\n",
      "\n",
      "  6,553.500 - 13,107: 11\n",
      "\n",
      " 13,107.000 - 19,660: 1\n",
      "\n",
      " 19,660.500 - 26,214: 20\n",
      "\n",
      " 26,214.000 - 32,768: 0\n",
      "\n",
      " 32,767.500 - 39,321: 0\n",
      "\n",
      " 39,321.000 - 45,874: 0\n",
      "\n",
      " 45,874.500 - 52,428: 0\n",
      "\n",
      " 52,428.000 - 58,982: 1\n",
      "\n",
      " 58,981.500 - 65,535: 2\n",
      "\n",
      "\n",
      "Column 5:\n",
      "\n",
      "          0.000 - 42: 79,210\n",
      "\n",
      "         41.700 - 83: 1,178\n",
      "\n",
      "        83.400 - 125: 44\n",
      "\n",
      "       125.100 - 167: 8\n",
      "\n",
      "       166.800 - 208: 12\n",
      "\n",
      "       208.500 - 250: 6\n",
      "\n",
      "       250.200 - 292: 5\n",
      "\n",
      "       291.900 - 334: 0\n",
      "\n",
      "       333.600 - 375: 0\n",
      "\n",
      "       375.300 - 417: 3\n",
      "\n",
      "\n",
      "Column 6:\n",
      "\n",
      "     0.000 - 174,113: 93,216\n",
      "\n",
      "174,112.800 - 348,226: 1,145\n",
      "\n",
      "348,225.600 - 522,338: 491\n",
      "\n",
      "522,338.400 - 696,451: 218\n",
      "\n",
      "696,451.200 - 870,564: 88\n",
      "\n",
      "870,564.000 - 1,044,677: 44\n",
      "\n",
      "1,044,676.800 - 1,218,790: 19\n",
      "\n",
      "1,218,789.600 - 1,392,902: 11\n",
      "\n",
      "1,392,902.400 - 1,567,015: 5\n",
      "\n",
      "1,567,015.200 - 1,741,128: 3\n",
      "\n",
      "\n",
      "Column 7:\n",
      "\n",
      "       0.000 - 1,629: 74,200\n",
      "\n",
      "   1,629.000 - 3,258: 536\n",
      "\n",
      "   3,258.000 - 4,887: 92\n",
      "\n",
      "   4,887.000 - 6,516: 36\n",
      "\n",
      "   6,516.000 - 8,145: 12\n",
      "\n",
      "   8,145.000 - 9,774: 10\n",
      "\n",
      "  9,774.000 - 11,403: 3\n",
      "\n",
      " 11,403.000 - 13,032: 1\n",
      "\n",
      " 13,032.000 - 14,661: 1\n",
      "\n",
      " 14,661.000 - 16,290: 2\n",
      "\n",
      "\n",
      "Column 8:\n",
      "\n",
      "         0.000 - 881: 95,218\n",
      "\n",
      "     880.700 - 1,761: 51\n",
      "\n",
      "   1,761.400 - 2,642: 8\n",
      "\n",
      "   2,642.100 - 3,523: 2\n",
      "\n",
      "   3,522.800 - 4,404: 0\n",
      "\n",
      "   4,403.500 - 5,284: 0\n",
      "\n",
      "   5,284.200 - 6,165: 1\n",
      "\n",
      "   6,164.900 - 7,046: 0\n",
      "\n",
      "   7,045.600 - 7,926: 0\n",
      "\n",
      "   7,926.300 - 8,807: 1\n",
      "\n",
      "\n",
      "Column 9:\n",
      "\n",
      "         0.000 - 468: 99,864\n",
      "\n",
      "       467.700 - 935: 11\n",
      "\n",
      "     935.400 - 1,403: 0\n",
      "\n",
      "   1,403.100 - 1,871: 0\n",
      "\n",
      "   1,870.800 - 2,338: 4\n",
      "\n",
      "   2,338.500 - 2,806: 4\n",
      "\n",
      "   2,806.200 - 3,274: 3\n",
      "\n",
      "   3,273.900 - 3,742: 1\n",
      "\n",
      "   3,741.600 - 4,209: 3\n",
      "\n",
      "   4,209.300 - 4,677: 3\n",
      "\n",
      "\n",
      "Column 10:\n",
      "\n",
      "       0.000 - 1,266: 94,344\n",
      "\n",
      "   1,266.100 - 2,532: 735\n",
      "\n",
      "   2,532.200 - 3,798: 144\n",
      "\n",
      "   3,798.300 - 5,064: 30\n",
      "\n",
      "   5,064.400 - 6,330: 12\n",
      "\n",
      "   6,330.500 - 7,597: 13\n",
      "\n",
      "   7,596.600 - 8,863: 1\n",
      "\n",
      "  8,862.700 - 10,129: 1\n",
      "\n",
      " 10,128.800 - 11,395: 0\n",
      "\n",
      " 11,394.900 - 12,661: 1\n",
      "\n",
      "\n",
      "Column 11:\n",
      "\n",
      "           0.000 - 1: 26,171\n",
      "\n",
      "           0.600 - 1: 25,277\n",
      "\n",
      "           1.200 - 2: 0\n",
      "\n",
      "           1.800 - 2: 3,409\n",
      "\n",
      "           2.400 - 3: 0\n",
      "\n",
      "           3.000 - 4: 580\n",
      "\n",
      "           3.600 - 4: 119\n",
      "\n",
      "           4.200 - 5: 0\n",
      "\n",
      "           4.800 - 5: 27\n",
      "\n",
      "           5.400 - 6: 4\n",
      "\n",
      "\n",
      "Column 12:\n",
      "\n",
      "          0.000 - 10: 91,416\n",
      "\n",
      "         10.400 - 21: 2,696\n",
      "\n",
      "         20.800 - 31: 765\n",
      "\n",
      "         31.200 - 42: 203\n",
      "\n",
      "         41.600 - 52: 82\n",
      "\n",
      "         52.000 - 62: 68\n",
      "\n",
      "         62.400 - 73: 26\n",
      "\n",
      "         72.800 - 83: 15\n",
      "\n",
      "         83.200 - 94: 4\n",
      "\n",
      "        93.600 - 104: 6\n",
      "\n",
      "\n",
      "Column 13:\n",
      "\n",
      "          0.000 - 49: 22,793\n",
      "\n",
      "         49.300 - 99: 21\n",
      "\n",
      "        98.600 - 148: 3\n",
      "\n",
      "       147.900 - 197: 2\n",
      "\n",
      "       197.200 - 246: 0\n",
      "\n",
      "       246.500 - 296: 0\n",
      "\n",
      "       295.800 - 345: 0\n",
      "\n",
      "       345.100 - 394: 0\n",
      "\n",
      "       394.400 - 444: 0\n",
      "\n",
      "       443.700 - 493: 1\n",
      "\n",
      "\n",
      "Column 14:\n",
      "\n",
      "         0.000 - 656: 80,441\n",
      "\n",
      "     655.800 - 1,312: 6\n",
      "\n",
      "   1,311.600 - 1,967: 5\n",
      "\n",
      "   1,967.400 - 2,623: 5\n",
      "\n",
      "   2,623.200 - 3,279: 3\n",
      "\n",
      "   3,279.000 - 3,935: 2\n",
      "\n",
      "   3,934.800 - 4,591: 3\n",
      "\n",
      "   4,590.600 - 5,246: 0\n",
      "\n",
      "   5,246.400 - 5,902: 0\n",
      "\n",
      "   5,902.200 - 6,558: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_bin(bins, index):\n",
    "    return \"{:0,.3f} - {:0,.0f}\".format(bins[i], bins[i + 1])\n",
    "\n",
    "\n",
    "for col_num, histogram in summary.int_columns_histograms_iter(train):\n",
    "    bins, counts = histogram\n",
    "    print(\"Column %d:\\n\" % col_num)\n",
    "    for i, count in enumerate(counts):\n",
    "        print (\"%20s: %s\\n\" % (pretty_print_bin(bins, i), \"{:,}\".format(count)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_counts = {col_name: counts for (col_name, counts) in summary.cat_column_counts_iter(train)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config.DEBUG:\n",
    "    for col_name in train.categorical_column_names:\n",
    "        counts = categorical_counts[col_name]\n",
    "        counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if config.DEBUG:\n",
    "    column_distinct_counts = {col_name: summary.column_distinct_count(train, col_name) \n",
    "                              for col_name in train.df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column C1: 541 distinct values\n",
      "Column C2: 497 distinct values\n",
      "Column C3: 43,870 distinct values\n",
      "Column C4: 25,184 distinct values\n",
      "Column C5: 145 distinct values\n",
      "Column C6: 12 distinct values\n",
      "Column C7: 7,623 distinct values\n",
      "Column C8: 257 distinct values\n",
      "Column C9: 3 distinct values\n",
      "Column C10: 10,997 distinct values\n",
      "Column C11: 3,799 distinct values\n",
      "Column C12: 41,312 distinct values\n",
      "Column C13: 2,796 distinct values\n",
      "Column C14: 26 distinct values\n",
      "Column C15: 5,238 distinct values\n",
      "Column C16: 34,617 distinct values\n",
      "Column C17: 10 distinct values\n",
      "Column C18: 2,548 distinct values\n",
      "Column C19: 1,303 distinct values\n",
      "Column C20: 4 distinct values\n",
      "Column C21: 38,618 distinct values\n",
      "Column C22: 11 distinct values\n",
      "Column C23: 14 distinct values\n",
      "Column C24: 12,335 distinct values\n",
      "Column C25: 51 distinct values\n",
      "Column C26: 9,527 distinct values\n"
     ]
    }
   ],
   "source": [
    "# Distinct value count for categorical features\n",
    "if config.DEBUG:\n",
    "    for col_name in train.categorical_column_names:\n",
    "        count = \"{:,}\".format(column_distinct_counts.get(col_name))\n",
    "        print(\"Column %s: %s distinct values\" % (col_name, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "integer_column_stats = {name: stats for (name, stats) in summary.integer_column_stats_iter(train)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1: \tskewness: 13.045\n",
      "\tkurtosis: 373.759\n",
      "\tstddev: 10.451\n",
      "\tmean: 3.769\n",
      "\n",
      "I2: \tskewness: 7.264\n",
      "\tkurtosis: 100.708\n",
      "\tstddev: 401.523\n",
      "\tmean: 112.864\n",
      "\n",
      "I3: \tskewness: 74.676\n",
      "\tkurtosis: 7,426.228\n",
      "\tstddev: 538.819\n",
      "\tmean: 40.745\n",
      "\n",
      "I4: \tskewness: 5.891\n",
      "\tkurtosis: 115.569\n",
      "\tstddev: 10.836\n",
      "\tmean: 8.280\n",
      "\n",
      "I5: \tskewness: 9.146\n",
      "\tkurtosis: 115.043\n",
      "\tstddev: 65,797.898\n",
      "\tmean: 17,592.599\n",
      "\n",
      "I6: \tskewness: 11.409\n",
      "\tkurtosis: 251.298\n",
      "\tstddev: 371.776\n",
      "\tmean: 139.685\n",
      "\n",
      "I7: \tskewness: 42.413\n",
      "\tkurtosis: 4,233.619\n",
      "\tstddev: 65.460\n",
      "\tmean: 15.222\n",
      "\n",
      "I8: \tskewness: 68.854\n",
      "\tkurtosis: 5,577.523\n",
      "\tstddev: 46.542\n",
      "\tmean: 13.575\n",
      "\n",
      "I9: \tskewness: 9.010\n",
      "\tkurtosis: 163.166\n",
      "\tstddev: 286.416\n",
      "\tmean: 125.295\n",
      "\n",
      "I10: \tskewness: 1.125\n",
      "\tkurtosis: 2.495\n",
      "\tstddev: 0.677\n",
      "\tmean: 0.620\n",
      "\n",
      "I11: \tskewness: 6.399\n",
      "\tkurtosis: 66.442\n",
      "\tstddev: 4.630\n",
      "\tmean: 2.400\n",
      "\n",
      "I12: \tskewness: 42.705\n",
      "\tkurtosis: 3,362.307\n",
      "\tstddev: 5.328\n",
      "\tmean: 0.938\n",
      "\n",
      "I13: \tskewness: 67.003\n",
      "\tkurtosis: 5,899.185\n",
      "\tstddev: 52.045\n",
      "\tmean: 11.608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_stats(stats):\n",
    "    return \"\\n\".join([\"\\t{}: {:0,.3f}\".format(key, val) for (key, val) in stats.items()])\n",
    "\n",
    "if config.DEBUG:\n",
    "    for col_name in train.integer_column_names:\n",
    "        stats = integer_column_stats[col_name]\n",
    "        print(\"%s: %s\\n\" % (col_name, pretty_print_stats(stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 100,000\n"
     ]
    }
   ],
   "source": [
    "row_count = summary.row_count(train)\n",
    "print(\"Row count: %s\" % \"{:,}\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count=0.12895),\n",
       " Row(count=0.06826),\n",
       " Row(count=0.04043),\n",
       " Row(count=0.03724),\n",
       " Row(count=0.03269),\n",
       " Row(count=0.03268),\n",
       " Row(count=0.02903),\n",
       " Row(count=0.02366),\n",
       " Row(count=0.01988),\n",
       " Row(count=0.01892),\n",
       " Row(count=0.01757),\n",
       " Row(count=0.01735),\n",
       " Row(count=0.01701),\n",
       " Row(count=0.01408),\n",
       " Row(count=0.01392),\n",
       " Row(count=0.01272),\n",
       " Row(count=0.01142),\n",
       " Row(count=0.01138),\n",
       " Row(count=0.01126),\n",
       " Row(count=0.01087),\n",
       " Row(count=0.00991),\n",
       " Row(count=0.00986),\n",
       " Row(count=0.00966),\n",
       " Row(count=0.00959),\n",
       " Row(count=0.00858),\n",
       " Row(count=0.0081),\n",
       " Row(count=0.00807),\n",
       " Row(count=0.00714),\n",
       " Row(count=0.00686),\n",
       " Row(count=0.0068),\n",
       " Row(count=0.00668),\n",
       " Row(count=0.00644),\n",
       " Row(count=0.00612),\n",
       " Row(count=0.00595),\n",
       " Row(count=0.00582),\n",
       " Row(count=0.00581),\n",
       " Row(count=0.00483),\n",
       " Row(count=0.00455),\n",
       " Row(count=0.00444),\n",
       " Row(count=0.00437),\n",
       " Row(count=0.00432),\n",
       " Row(count=0.00427),\n",
       " Row(count=0.00422),\n",
       " Row(count=0.00398),\n",
       " Row(count=0.00393),\n",
       " Row(count=0.00385),\n",
       " Row(count=0.00382),\n",
       " Row(count=0.00378),\n",
       " Row(count=0.00364),\n",
       " Row(count=0.00359),\n",
       " Row(count=0.00358),\n",
       " Row(count=0.00355),\n",
       " Row(count=0.00337),\n",
       " Row(count=0.00324),\n",
       " Row(count=0.00317),\n",
       " Row(count=0.00317),\n",
       " Row(count=0.00313),\n",
       " Row(count=0.00312),\n",
       " Row(count=0.00305),\n",
       " Row(count=0.00302),\n",
       " Row(count=0.00295),\n",
       " Row(count=0.00293),\n",
       " Row(count=0.00283),\n",
       " Row(count=0.0027),\n",
       " Row(count=0.00266),\n",
       " Row(count=0.0026),\n",
       " Row(count=0.00259),\n",
       " Row(count=0.00253),\n",
       " Row(count=0.00252),\n",
       " Row(count=0.0025),\n",
       " Row(count=0.00249),\n",
       " Row(count=0.00248),\n",
       " Row(count=0.00243),\n",
       " Row(count=0.00241),\n",
       " Row(count=0.00239),\n",
       " Row(count=0.00235),\n",
       " Row(count=0.00232),\n",
       " Row(count=0.00224),\n",
       " Row(count=0.00218),\n",
       " Row(count=0.00212),\n",
       " Row(count=0.0021),\n",
       " Row(count=0.00203),\n",
       " Row(count=0.00198),\n",
       " Row(count=0.00196),\n",
       " Row(count=0.00195),\n",
       " Row(count=0.00194),\n",
       " Row(count=0.0019),\n",
       " Row(count=0.00184),\n",
       " Row(count=0.00183),\n",
       " Row(count=0.00181),\n",
       " Row(count=0.0018),\n",
       " Row(count=0.00177),\n",
       " Row(count=0.00175),\n",
       " Row(count=0.00175),\n",
       " Row(count=0.00175),\n",
       " Row(count=0.0017),\n",
       " Row(count=0.00157),\n",
       " Row(count=0.00156),\n",
       " Row(count=0.00153),\n",
       " Row(count=0.00153)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "col_name_test = \"C2\"\n",
    "\n",
    "cc = categorical_counts[col_name_test]\n",
    "cc.select([cc[col_name_test], cc[\"count\"].cast(DoubleType())]).map(lambda r: Row(count=r[\"count\"] / row_count)).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_counts = features.join_column_counts(train, categorical_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9bcebbeafa78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_counts' is not defined"
     ]
    }
   ],
   "source": [
    "df_counts.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_counts.map(lambda row: row.C1_count / row_count).take(1)\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# from pyspark.sql.functions import udf\n",
    "\n",
    "# def make_rate_udf(denominator):\n",
    "#     return udf(lambda val: val / denominator)\n",
    "\n",
    "# rate = make_rate_udf(row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sqlc = config.SPARK_SQL_CONTEXT\n",
    "# sqlc.registerFunction(\"rate\", rate, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1103.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 611.0 failed 1 times, most recent failure: Lost task 1.0 in stage 611.0 (TID 45200, localhost): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.String.substring(String.java:1969)\n\tat java.lang.String.subSequence(String.java:2003)\n\tat java.util.regex.Matcher.getSubSequence(Matcher.java:1294)\n\tat java.util.regex.Matcher.group(Matcher.java:541)\n\tat org.apache.spark.network.util.JavaUtils.parseByteString(JavaUtils.java:222)\n\tat org.apache.spark.network.util.JavaUtils.byteStringAsBytes(JavaUtils.java:255)\n\tat org.apache.spark.util.Utils$.byteStringAsBytes(Utils.scala:1001)\n\tat org.apache.spark.SparkConf.getSizeAsBytes(SparkConf.scala:249)\n\tat org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:160)\n\tat org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1179)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:91)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:212)\n\tat org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1537)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1544)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1414)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1413)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2138)\n\tat org.apache.spark.sql.DataFrame.head(DataFrame.scala:1413)\n\tat org.apache.spark.sql.DataFrame.take(DataFrame.scala:1495)\n\tat org.apache.spark.sql.DataFrame.showString(DataFrame.scala:171)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.String.substring(String.java:1969)\n\tat java.lang.String.subSequence(String.java:2003)\n\tat java.util.regex.Matcher.getSubSequence(Matcher.java:1294)\n\tat java.util.regex.Matcher.group(Matcher.java:541)\n\tat org.apache.spark.network.util.JavaUtils.parseByteString(JavaUtils.java:222)\n\tat org.apache.spark.network.util.JavaUtils.byteStringAsBytes(JavaUtils.java:255)\n\tat org.apache.spark.util.Utils$.byteStringAsBytes(Utils.scala:1001)\n\tat org.apache.spark.SparkConf.getSizeAsBytes(SparkConf.scala:249)\n\tat org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:160)\n\tat org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1179)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:91)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-5283dccfad6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"C1_count\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C1_rate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# df_counts.select(\"C1_count\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sean/bin/spark/current/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \"\"\"\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sean/.virtualenvs/datascience/local/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 835\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sean/bin/spark/current/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sean/.virtualenvs/datascience/local/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    309\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1103.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 611.0 failed 1 times, most recent failure: Lost task 1.0 in stage 611.0 (TID 45200, localhost): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.String.substring(String.java:1969)\n\tat java.lang.String.subSequence(String.java:2003)\n\tat java.util.regex.Matcher.getSubSequence(Matcher.java:1294)\n\tat java.util.regex.Matcher.group(Matcher.java:541)\n\tat org.apache.spark.network.util.JavaUtils.parseByteString(JavaUtils.java:222)\n\tat org.apache.spark.network.util.JavaUtils.byteStringAsBytes(JavaUtils.java:255)\n\tat org.apache.spark.util.Utils$.byteStringAsBytes(Utils.scala:1001)\n\tat org.apache.spark.SparkConf.getSizeAsBytes(SparkConf.scala:249)\n\tat org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:160)\n\tat org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1179)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:91)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:212)\n\tat org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1538)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2125)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1537)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1544)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1414)\n\tat org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1413)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2138)\n\tat org.apache.spark.sql.DataFrame.head(DataFrame.scala:1413)\n\tat org.apache.spark.sql.DataFrame.take(DataFrame.scala:1495)\n\tat org.apache.spark.sql.DataFrame.showString(DataFrame.scala:171)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.String.substring(String.java:1969)\n\tat java.lang.String.subSequence(String.java:2003)\n\tat java.util.regex.Matcher.getSubSequence(Matcher.java:1294)\n\tat java.util.regex.Matcher.group(Matcher.java:541)\n\tat org.apache.spark.network.util.JavaUtils.parseByteString(JavaUtils.java:222)\n\tat org.apache.spark.network.util.JavaUtils.byteStringAsBytes(JavaUtils.java:255)\n\tat org.apache.spark.util.Utils$.byteStringAsBytes(Utils.scala:1001)\n\tat org.apache.spark.SparkConf.getSizeAsBytes(SparkConf.scala:249)\n\tat org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:160)\n\tat org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1179)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.BlockManager$$anonfun$9.apply(BlockManager.scala:659)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:91)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 32837)\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 295, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 321, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 334, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python2.7/SocketServer.py\", line 649, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/sean/bin/spark/current/python/pyspark/accumulators.py\", line 235, in handle\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/sean/bin/spark/current/python/pyspark/serializers.py\", line 545, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "# df_counts.select(rate(df_counts[\"C1_count\"]).alias('C1_rate')).show()\n",
    "# df_counts.select(\"C1_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull just the mean values from the integer column stats\n",
    "integer_column_means = {col_name: round(integer_column_stats[col_name][\"mean\"])\n",
    "                        for col_name in train_3m.integer_column_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace nulls with averages for the column before building feature array\n",
    "train_filled_df = train_3m.df.fillna(integer_column_means)\n",
    "\n",
    "train_3m.df.select(train_3m.integer_column_names).show(10)\n",
    "train_filled_df.select(train_3m.integer_column_names).show(10)\n",
    "train_filled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=target_df_ints.select(int_column_names).columns, outputCol=\"features\")\n",
    "target_df_ints = vecAssembler.transform(target_df_ints)\n",
    "target_df_ints.show(5)\n",
    "target_df.show(5)\n",
    "\n",
    "# target_df.unionAll(int_featutes_df.select('features'))\n",
    "# int_featutes_df.head(5)\n",
    "\n",
    "# target_df.show(5)\n",
    "\n",
    "# target_df.withColumn('features', vecAssembler.transform(target_df_ints).features)\n",
    "\n",
    "# int_rows = target_df_ints.map(lambda row: row.asDict().values()).map(Vectors.dense).map(Row)\n",
    "# int_features_rdd.take(5)\n",
    "\n",
    "# int_features_df = sqlContext.createDataFrame(int_features_rdd)\n",
    "\n",
    "# column_scaler_models = {\n",
    "#     col_name: standard_scale_column(target_df, col_name) \n",
    "#     for i, col_name in enumerate(target_df.columns)\n",
    "#     if is_integer_col_num(i + 1)\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_scale_column(df, col_name):\n",
    "    output_col_name = \"%s%s\" % (col_name, \"_scaled\")\n",
    "    scaler = StandardScaler(inputCol=col_name, outputCol=output_col_name)\n",
    "    return scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_sscaler_model = standard_scale_column(target_df_ints, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_df_ints.select(\"features\").show(5)\n",
    "int_sscaler_model.transform(target_df_ints).select(\"features_scaled\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
