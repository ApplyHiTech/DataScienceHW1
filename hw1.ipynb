{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Full original training set\n",
    "raw_full_train = sc.textFile(\"dac/train.txt\")\n",
    "\n",
    "# Final test set split\n",
    "raw_final_test = sc.textFile(\"dac/split/test.txt\")  # Do not touch during training\n",
    "\n",
    "# Training set splits\n",
    "raw_test_3m = sc.textFile(\"dac/split/test_3m.txt\")\n",
    "raw_train_5m = sc.textFile(\"dac/split/train_5m.txt\")\n",
    "raw_validation_2m = sc.textFile(\"dac/split/train_5m.txt\")\n",
    "\n",
    "# Debug set\n",
    "raw_small_train = sc.textFile(\"dac/small-train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o284.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7, localhost): org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253)\n\tat java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)\n\tat java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145)\n\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.write(GzipCodec.java:90)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.write(TextOutputFormat.java:104)\n\tat org.apache.spark.SparkHadoopWriter.write(SparkHadoopWriter.scala:96)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1199)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1250)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1205)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\t\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.util.zip.DeflaterOutputStream.flush(DeflaterOutputStream.java:282)\n\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.flush(GzipCodec.java:79)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\n\t\tat org.apache.spark.SparkHadoopWriter.close(SparkHadoopWriter.scala:103)\n\t\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1206)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1259)\n\t\t... 8 more\n\t\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:168)\n\t\t\tat java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)\n\t\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.close(GzipCodec.java:74)\n\t\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t\t... 12 more\n\t\tCaused by: java.io.IOException: No space left on device\n\t\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t\t... 28 more\n\tCaused by: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t... 26 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t... 32 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1006)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:964)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1461)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1449)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:515)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253)\n\tat java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)\n\tat java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145)\n\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.write(GzipCodec.java:90)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.write(TextOutputFormat.java:104)\n\tat org.apache.spark.SparkHadoopWriter.write(SparkHadoopWriter.scala:96)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1199)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1250)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1205)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\t\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.util.zip.DeflaterOutputStream.flush(DeflaterOutputStream.java:282)\n\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.flush(GzipCodec.java:79)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\n\t\tat org.apache.spark.SparkHadoopWriter.close(SparkHadoopWriter.scala:103)\n\t\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1206)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1259)\n\t\t... 8 more\n\t\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:168)\n\t\t\tat java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)\n\t\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.close(GzipCodec.java:74)\n\t\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t\t... 12 more\n\t\tCaused by: java.io.IOException: No space left on device\n\t\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t\t... 28 more\n\tCaused by: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t... 26 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-636674cfc759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_test_3m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dac/split/test_3m_compressed.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.hadoop.io.compress.GzipCodec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sean/bin/spark/current/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompressionCodecClass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mcompressionCodec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressionCodecClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sean/bin/spark/current/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sean/bin/spark/current/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sean/bin/spark/current/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o284.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7, localhost): org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253)\n\tat java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)\n\tat java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145)\n\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.write(GzipCodec.java:90)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.write(TextOutputFormat.java:104)\n\tat org.apache.spark.SparkHadoopWriter.write(SparkHadoopWriter.scala:96)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1199)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1250)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1205)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\t\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.util.zip.DeflaterOutputStream.flush(DeflaterOutputStream.java:282)\n\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.flush(GzipCodec.java:79)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\n\t\tat org.apache.spark.SparkHadoopWriter.close(SparkHadoopWriter.scala:103)\n\t\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1206)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1259)\n\t\t... 8 more\n\t\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:168)\n\t\t\tat java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)\n\t\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.close(GzipCodec.java:74)\n\t\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t\t... 12 more\n\t\tCaused by: java.io.IOException: No space left on device\n\t\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t\t... 28 more\n\tCaused by: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t... 26 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t... 32 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1006)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:964)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1461)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1449)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:515)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:253)\n\tat java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211)\n\tat java.util.zip.GZIPOutputStream.write(GZIPOutputStream.java:145)\n\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.write(GzipCodec.java:90)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.write(TextOutputFormat.java:104)\n\tat org.apache.spark.SparkHadoopWriter.write(SparkHadoopWriter.scala:96)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1199)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1197)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1250)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1205)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1185)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\t\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.util.zip.DeflaterOutputStream.flush(DeflaterOutputStream.java:282)\n\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.flush(GzipCodec.java:79)\n\t\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\n\t\tat org.apache.spark.SparkHadoopWriter.close(SparkHadoopWriter.scala:103)\n\t\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1206)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1259)\n\t\t... 8 more\n\t\tSuppressed: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:226)\n\t\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\t\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:412)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:144)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:135)\n\t\t\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\t\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\t\t\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\t\t\tat java.util.zip.GZIPOutputStream.finish(GZIPOutputStream.java:168)\n\t\t\tat java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:238)\n\t\t\tat org.apache.hadoop.io.compress.GzipCodec$GzipOutputStream.close(GzipCodec.java:74)\n\t\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t\t... 12 more\n\t\tCaused by: java.io.IOException: No space left on device\n\t\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t\t... 28 more\n\tCaused by: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t\t... 26 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:315)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:224)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "raw_test_3m.saveAsTextFile(\"dac/split/test_3m_compressed.txt\", \"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_value(index, value):\n",
    "    if index < 14:\n",
    "        return int(value) if value else None\n",
    "    else:\n",
    "        return value if value else None\n",
    "\n",
    "def convert_line(line):\n",
    "    return [convert_value(i, value) for i, value in enumerate(line.split(\"\\t\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_train = raw_full_train.map(convert_line)\n",
    "final_test = raw_final_test.map(convert_line)\n",
    "test_3m = raw_test_3m.map(convert_line)\n",
    "train_5m = raw_train_5m.map(convert_line)\n",
    "validation_2m = raw_validation_2m.map(convert_line)\n",
    "\n",
    "debug = raw_small_train.map(convert_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2], [77337, 22663])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug.map(lambda row: row[0]).histogram([0,1,2])\n",
    "# data.map(lambda row: row[0]).histogram([0, 1, 2])\n",
    "# data.map(lambda row: row[2]).histogram(10)\n",
    "# a = data.map(lambda row: row[2]).histogram([-10,0,5,10,50,100,200,500,1000,2000,10000,50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column #1: bins=[0.0, 55.6, 111.2, 166.8, 222.4, 278.0, 333.6, 389.2, 444.8, 500.40000000000003, 556], counts=[55236, 295, 34, 10, 5, 3, 2, 0, 1, 1] (total=55587)\n",
      "column #2: bins=[-2.0, 1850.4, 3702.8, 5555.200000000001, 7407.6, 9260.0, 11112.400000000001, 12964.800000000001, 14817.2, 16669.600000000002, 18522], counts=[98530, 1397, 52, 12, 7, 0, 1, 0, 0, 1] (total=100000)\n",
      "column #3: bins=[0.0, 6553.5, 13107.0, 19660.5, 26214.0, 32767.5, 39321.0, 45874.5, 52428.0, 58981.5, 65535], counts=[80863, 11, 1, 20, 0, 0, 0, 0, 1, 2] (total=80898)\n",
      "column #4: bins=[0.0, 41.7, 83.4, 125.10000000000001, 166.8, 208.5, 250.20000000000002, 291.90000000000003, 333.6, 375.3, 417], counts=[79210, 1178, 44, 8, 12, 6, 5, 0, 0, 3] (total=80466)\n",
      "column #5: bins=[0.0, 174112.8, 348225.6, 522338.39999999997, 696451.2, 870564.0, 1044676.7999999999, 1218789.5999999999, 1392902.4, 1567015.2, 1741128], counts=[93216, 1145, 491, 218, 88, 44, 19, 11, 5, 3] (total=95240)\n",
      "column #6: bins=[0, 1629, 3258, 4887, 6516, 8145, 9774, 11403, 13032, 14661, 16290], counts=[74200, 536, 92, 36, 12, 10, 3, 1, 1, 2] (total=74893)\n",
      "column #7: bins=[0.0, 880.7, 1761.4, 2642.1000000000004, 3522.8, 4403.5, 5284.200000000001, 6164.900000000001, 7045.6, 7926.3, 8807], counts=[95218, 51, 8, 2, 0, 0, 1, 0, 0, 1] (total=95281)\n",
      "column #8: bins=[0.0, 467.7, 935.4, 1403.1, 1870.8, 2338.5, 2806.2, 3273.9, 3741.6, 4209.3, 4677], counts=[99864, 11, 0, 0, 4, 4, 3, 1, 3, 3] (total=99893)\n",
      "column #9: bins=[0.0, 1266.1, 2532.2, 3798.2999999999997, 5064.4, 6330.5, 7596.599999999999, 8862.699999999999, 10128.8, 11394.9, 12661], counts=[94344, 735, 144, 30, 12, 13, 1, 1, 0, 1] (total=95281)\n",
      "column #10: bins=[0.0, 0.6, 1.2, 1.7999999999999998, 2.4, 3.0, 3.5999999999999996, 4.2, 4.8, 5.3999999999999995, 6], counts=[26171, 25277, 0, 3409, 0, 580, 119, 0, 27, 4] (total=55587)\n",
      "column #11: bins=[0.0, 10.4, 20.8, 31.200000000000003, 41.6, 52.0, 62.400000000000006, 72.8, 83.2, 93.60000000000001, 104], counts=[91416, 2696, 765, 203, 82, 68, 26, 15, 4, 6] (total=95281)\n",
      "column #12: bins=[0.0, 49.3, 98.6, 147.89999999999998, 197.2, 246.5, 295.79999999999995, 345.09999999999997, 394.4, 443.7, 493], counts=[22793, 21, 3, 2, 0, 0, 0, 0, 0, 1] (total=22820)\n",
      "column #13: bins=[0.0, 655.8, 1311.6, 1967.3999999999999, 2623.2, 3279.0, 3934.7999999999997, 4590.599999999999, 5246.4, 5902.2, 6558], counts=[80441, 6, 5, 5, 3, 2, 3, 0, 0, 1] (total=80466)\n"
     ]
    }
   ],
   "source": [
    "def show_histogram(col_num, data):\n",
    "    bins, counts = data.map(lambda row: row[col_num]).histogram(10)\n",
    "    total = sum(counts)\n",
    "    print \"column #%d: bins=%s, counts=%s (total=%d)\" % (col_num, bins, counts, total)\n",
    "    #sum the counts\n",
    "    #max of the counts\n",
    "    #if  > 25%\n",
    "    \n",
    "for i in range(1,14):\n",
    "    show_histogram(i, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
